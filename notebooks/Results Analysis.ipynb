{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = sorted([json.loads(line) for line in open('../model_data/pwc_split_on_sectioned/dev.jsonl')], key=lambda x : x['doc_id'])\n",
    "spans = sorted([json.loads(line) for line in open('../outputs/n_ary_test/spans.jsonl')], key=lambda x : x['doc_id'])\n",
    "corefs = sorted([json.loads(line) for line in open('../outputs/n_ary_test/coref.jsonl')], key=lambda x : x['doc_id'])\n",
    "clusters = sorted([json.loads(line) for line in open('../outputs/n_ary_test/clusters.jsonl')], key=lambda x : x['doc_id'])\n",
    "n_ary_relations = sorted([json.loads(line) for line in open('../outputs/n_ary_test/n_ary_relations.jsonl')], key=lambda x : x['doc_id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 ['sequence transduction models', 'neural sequence transduction models', 'sequence transduction models', 'sequence transduction encoder or decoder', 'sequence transduction model']\n",
      "1 ['multi - head self - attention mechanism', 'Multi - Head Attention', 'Scaled Dot - Product Attention Multi - Head Attention', 'Multi - head attention', 'single - head attention', 'multi - head attention', 'single - head attention']\n",
      "2 ['sequence modeling', 'sequence - to - sequence models']\n",
      "3 ['English - to - German translation', 'translation tasks']\n",
      "4 ['position - wise fully connected feed - forward network', 'feed - forward network', 'Position - wise Feed - Forward Networks', 'fully connected feed - forward network', 'point - wise feed - forward layer']\n",
      "5 ['attention head', 'Attention Visualizations']\n",
      "6 ['WMT 2014 English - to - German translation task', 'WMT 2014 English - to - French translation task', 'WMT 2014 English - to - German translation task', 'WMT 2014 English - to - French translation task', 'WMT 2014 English - to - German', 'WMT 2014 English - to - French translation tasks']\n",
      "7 ['translation quality', 'translation quality', 'quality', 'model quality']\n",
      "8 ['Transformer', 'Transformer', 'Transformer', 'Transformer', 'Transformer', 'Transformer', 'Transformer', 'Transformer', 'Transformer', 'Transformer', '4-layer transformer', 'Transformer', 'Transformer', 'Transformer', 'Transformer']\n",
      "9 ['GPUs', 'GPU']\n",
      "10 ['symbol representations', 'symbol representations', 'sentence representations']\n",
      "11 ['learning task - independent sentence representations', 'Learning long - range dependencies']\n",
      "12 ['Scaled Dot - Product Attention', 'Scaled Dot - Product Attention', 'dot - product ( multiplicative ) attention', 'Dot - product attention', 'dot - product attention', 'dot product attention', 'scaled dot - product attention', 'dot product']\n",
      "13 ['encoder - decoder attention \" layers', 'encoder - decoder attention mechanisms']\n",
      "14 ['Positional Encoding', 'positional encodings', 'positional encodings', 'positional encodings', 'positional encoding', 'positional encodings', 'sinusoidal positional encoding']\n",
      "15 ['newstest2013', 'Wall Street Journal']\n",
      "16 ['recurrent and convolutional layers', 'recurrent or convolutional layers']\n",
      "17 ['recurrent or convolutional neural networks', 'convolutional neural networks']\n",
      "18 ['computational efficiency', 'computational cost', 'computational complexity', 'computational complexity', 'computational performance']\n",
      "19 ['encoder layer', 'hidden layer', 'hidden layer']\n",
      "20 ['Residual Dropout', 'dropout', 'dropout', 'dropout']\n",
      "21 ['Recurrent neural networks', 'recurrent network', 'Recurrent Neural Network Grammar']\n",
      "22 ['sub - layer', 'attention sub - layers']\n",
      "23 ['encoder - decoder architectures', 'encoder - decoder structure', 'encoder - decoder architectures']\n",
      "24 ['WSJ only setting', 'WSJ training set']\n",
      "25 ['Self - attention', 'Self - attention', 'self - attention', 'stacked self - attention', 'self - attention layers', 'self - attention layer', 'self - attention layers', 'Self - Attention', 'self - attention layers', 'self - attention', 'self - attention layer', 'self - attention layers', 'self - attention layer', 'multi - headed self - attention']\n",
      "26 ['ConvS2S JonasFaceNet2017', 'ConvS2S']\n",
      "27 ['English - French', 'Penn Treebank']\n",
      "28 ['beam search', 'beam search']\n",
      "29 ['Separable convolutions', 'separable convolution']\n",
      "30 ['ByteNet', 'ByteNet']\n",
      "31 ['attention mechanism', 'attention mechanisms', 'Attention mechanisms', 'attention mechanisms', 'attention mechanism', 'attention mechanism', 'attention', 'attention']\n",
      "32 ['model architecture', 'Model Architecture', 'model architectures']\n",
      "33 ['softmax function', 'Softmax', 'softmax function']\n",
      "34 ['byte - pair encoding']\n",
      "35 ['sequence modeling', 'sequence transduction tasks']\n",
      "36 ['linear transformations', 'learned linear transformation']\n",
      "37 ['sinusoidal version']\n",
      "38 ['language modeling', 'language modeling tasks']\n",
      "39 ['English constituency parsing', 'English Constituency Parsing', 'English constituency parsing']\n",
      "40 ['convolutions', 'convolution', 'convolutions', 'convolution']\n",
      "41 ['additive attention', 'Additive attention', 'additive attention']\n",
      "42 ['recurrent layer', 'recurrent layers', 'recurrent layers', 'recurrent layers']\n",
      "43 ['learning rate', 'learning rate', 'learning rates']\n",
      "44 ['attention - based models']\n",
      "45 ['sequence - aligned recurrence', 'sequence - aligned RNNs']\n",
      "46 ['convolutional layer', 'Convolutional layers']\n",
      "47 ['over - fitting']\n",
      "48 ['RNN sequence - to - sequence models', 'RNN sequence - to - sequence models']\n",
      "49 ['dilated convolutions']\n",
      "50 ['big transformer model', 'Transformer ( big', 'Transformer ( big ) model']\n",
      "51 ['transduction models', 'transduction model']\n",
      "52 ['Extended Neural GPU', 'neural_gpu']\n",
      "53 ['machine translation tasks', 'machine translation', 'machine translations', 'Machine Translation']\n",
      "54 ['semi - supervised setting', 'semi - supervised setting']\n",
      "55 ['sequential computation', 'sequential computation']\n",
      "56 ['attention function', 'attention function', 'attention functions', 'attention function', 'attention function']\n",
      "57 ['training costs', 'training cost', 'training cost', 'training costs']\n",
      "58 ['English - to - German base translation model']\n",
      "59 ['self - attention']\n",
      "60 ['Adam optimizer kingma2014adam']\n",
      "61 ['matrix multiplication code']\n",
      "62 ['interpretable models']\n",
      "63 ['generation']\n",
      "64 ['BLEU', 'BLEU', 'BLEU score', 'BLEU score', 'BLEU', 'BLEU score', 'BLEU score', 'BLEU']\n",
      "65 ['compatibility function']\n",
      "66 ['Regularization', 'regularization']\n",
      "67 ['point - wise , fully connected layers']\n",
      "68 ['encoder', 'decoder', 'encoder and decoder', 'encoder', 'decoder', 'encoder and decoder', 'Encoder', 'encoder', 'Decoder', 'decoder', 'decoder', 'encoder', 'encoder', 'encoder', 'encoder', 'encoder', 'decoder']\n",
      "69 ['parallel attention layers']\n",
      "70 ['reading comprehension']\n",
      "71 ['stack of identical layers', 'stack of identical layers']\n",
      "72 ['parallelization']\n",
      "73 ['effective resolution']\n",
      "74 ['gated recurrent gruEval14 neural networks']\n",
      "75 ['long short - term memory hochreiter1997']\n",
      "76 ['continuous representations']\n",
      "77 ['recurrence', 'recurrence']\n",
      "78 ['ensembles']\n",
      "79 ['factorization tricks']\n",
      "80 ['building block']\n",
      "81 ['accuracy']\n",
      "82 ['theoretical complexity', 'complexity', 'complexity']\n",
      "83 ['base models']\n",
      "84 ['local , restricted attention mechanisms']\n",
      "85 ['inference']\n",
      "86 ['layer normalization', 'layer normalization']\n",
      "87 ['network architecture']\n",
      "88 ['BerkeleyParser']\n",
      "89 ['End - to - end memory networks']\n",
      "90 ['textual entailment']\n",
      "91 ['ReLU activation']\n",
      "92 ['WMT 2014 English - German dataset', 'WMT 2014 English - French dataset']\n",
      "93 ['NVIDIA P100 GPUs']\n",
      "94 ['Label Smoothing', 'label smoothing']\n",
      "95 ['hidden representations']\n",
      "96 ['intra - attention']\n",
      "97 ['embedding layers', 'embedding layers', 'embedding layers']\n",
      "98 ['recurrent attention mechanism']\n",
      "99 ['abstractive summarization']\n",
      "100 ['pre - softmax linear transformation']\n",
      "101 ['auto - regressive']\n",
      "102 ['value DBLP']\n",
      "103 ['conditional computation']\n",
      "104 ['transduction problems']\n",
      "105 ['Encoder and Decoder Stacks', 'encoder and decoder', 'encoder and decoder stacks', 'encoder and decoder stacks']\n",
      "106 ['byte - pair sennrich2015neural representations']\n",
      "107 ['big models,(described', 'big models', 'big model', 'big models']\n",
      "108 ['Optimizer']\n",
      "109 ['recurrent language models']\n",
      "110 ['word - piece wu2016google']\n",
      "111 ['checkpoint averaging']\n",
      "112 ['dropout rate']\n",
      "113 ['Recurrent models']\n",
      "114 ['simple - language question answering']\n",
      "115 ['stack of convolutional layers']\n"
     ]
    }
   ],
   "source": [
    "map_clusters_to_names = {}\n",
    "for k, v in clusters[11]['coref'].items() :\n",
    "    items = [\" \".join(clusters[11]['words'][s[0]:s[1]]) for s in v]\n",
    "    print(k, items)\n",
    "    map_clusters_to_names[k] = max(items, key=lambda x : len(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2548"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(n_ary_relations[11]['scores'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['IWSLT2015_English-German',\n",
       "  'BLEU_score',\n",
       "  'Machine_Translation',\n",
       "  'Transformer',\n",
       "  '28.23'],\n",
       " ['IWSLT2015_German-English',\n",
       "  'BLEU_score',\n",
       "  'Machine_Translation',\n",
       "  'Transformer',\n",
       "  '34.44'],\n",
       " ['Penn_Treebank', 'F1_score', 'Constituency_Parsing', 'Transformer', '92.7'],\n",
       " ['WMT2014_English-French',\n",
       "  'BLEU_score',\n",
       "  'Machine_Translation',\n",
       "  'Transformer_Base',\n",
       "  '38.1'],\n",
       " ['WMT2014_English-French',\n",
       "  'BLEU_score',\n",
       "  'Machine_Translation',\n",
       "  'Transformer_Big',\n",
       "  '41.0'],\n",
       " ['WMT2014_English-German',\n",
       "  'BLEU_score',\n",
       "  'Machine_Translation',\n",
       "  'Transformer_Base',\n",
       "  '27.3'],\n",
       " ['WMT2014_English-German',\n",
       "  'BLEU_score',\n",
       "  'Machine_Translation',\n",
       "  'Transformer_Big',\n",
       "  '28.4']]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "documents[11]['n_ary_relations']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['WMT 2014 English - German dataset', 'training costs', 'semi - supervised setting', 'byte - pair sennrich2015neural representations']\n",
      "['WMT 2014 English - German dataset', 'BLEU score', 'English constituency parsing', 'sinusoidal positional encoding']\n",
      "['WMT 2014 English - German dataset', 'BLEU score', 'semi - supervised setting', 'byte - pair sennrich2015neural representations']\n",
      "['WMT 2014 English - German dataset', 'BLEU score', 'semi - supervised setting', 'sinusoidal positional encoding']\n",
      "['WMT 2014 English - German dataset', 'BLEU score', 'semi - supervised setting', 'Attention Visualizations']\n",
      "['WMT 2014 English - German dataset', 'BLEU score', 'reading comprehension', 'sinusoidal positional encoding']\n",
      "['WSJ only setting', 'training costs', 'semi - supervised setting', 'byte - pair sennrich2015neural representations']\n",
      "['WSJ only setting', 'training costs', 'semi - supervised setting', 'word - piece wu2016google']\n",
      "['WSJ only setting', 'training costs', 'sequential computation', 'byte - pair sennrich2015neural representations']\n",
      "['WSJ only setting', 'training costs', 'self - attention', 'byte - pair sennrich2015neural representations']\n",
      "['WSJ only setting', 'BLEU score', 'English - to - German translation', 'sinusoidal positional encoding']\n",
      "['WSJ only setting', 'BLEU score', 'sequence transduction tasks', 'sinusoidal positional encoding']\n",
      "['WSJ only setting', 'BLEU score', 'language modeling tasks', 'byte - pair sennrich2015neural representations']\n",
      "['WSJ only setting', 'BLEU score', 'English constituency parsing', 'byte - pair sennrich2015neural representations']\n",
      "['WSJ only setting', 'BLEU score', 'English constituency parsing', 'sinusoidal positional encoding']\n",
      "['WSJ only setting', 'BLEU score', 'English constituency parsing', 'Attention Visualizations']\n",
      "['WSJ only setting', 'BLEU score', 'semi - supervised setting', 'byte - pair sennrich2015neural representations']\n",
      "['WSJ only setting', 'BLEU score', 'semi - supervised setting', 'word - piece wu2016google']\n",
      "['WSJ only setting', 'BLEU score', 'semi - supervised setting', 'dot - product ( multiplicative ) attention']\n",
      "['WSJ only setting', 'BLEU score', 'semi - supervised setting', 'sinusoidal positional encoding']\n",
      "['WSJ only setting', 'BLEU score', 'semi - supervised setting', 'byte - pair encoding']\n",
      "['WSJ only setting', 'BLEU score', 'semi - supervised setting', 'Attention Visualizations']\n",
      "['WSJ only setting', 'BLEU score', 'sequential computation', 'byte - pair sennrich2015neural representations']\n",
      "['WSJ only setting', 'BLEU score', 'sequential computation', 'sinusoidal positional encoding']\n",
      "['WSJ only setting', 'BLEU score', 'sequential computation', 'Attention Visualizations']\n",
      "['WSJ only setting', 'BLEU score', 'self - attention', 'word - piece wu2016google']\n",
      "['WSJ only setting', 'BLEU score', 'self - attention', 'dot - product ( multiplicative ) attention']\n",
      "['WSJ only setting', 'BLEU score', 'self - attention', 'sinusoidal positional encoding']\n",
      "['WSJ only setting', 'BLEU score', 'self - attention', 'Attention Visualizations']\n",
      "['WSJ only setting', 'BLEU score', 'reading comprehension', 'byte - pair sennrich2015neural representations']\n",
      "['WSJ only setting', 'BLEU score', 'reading comprehension', 'sinusoidal positional encoding']\n",
      "['WSJ only setting', 'BLEU score', 'reading comprehension', 'Attention Visualizations']\n",
      "['English - French', 'training costs', 'semi - supervised setting', 'sinusoidal positional encoding']\n",
      "['English - French', 'BLEU score', 'semi - supervised setting', 'sinusoidal positional encoding']\n",
      "['Wall Street Journal', 'BLEU score', 'English constituency parsing', 'Attention Visualizations']\n",
      "['Wall Street Journal', 'BLEU score', 'self - attention', 'byte - pair encoding']\n"
     ]
    }
   ],
   "source": [
    "for rel, s in zip(n_ary_relations[11]['predicted_relations'], n_ary_relations[11]['scores']) :\n",
    "    if s > 0.70 :\n",
    "        print([map_clusters_to_names[r] for r in rel])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "n_ary_relations[11]['predicted_relations']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dygie.data.dataset_readers.paragraph_utils import get_features_for_sections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_features_for_sections(documents[0]['sections'], documents[0]['words'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents[0]['section_heads']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
