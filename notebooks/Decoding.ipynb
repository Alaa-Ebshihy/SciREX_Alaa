{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%config InlineBackend.figure_format = 'retina'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scripts.entity_utils import *\n",
    "import pandas as pd\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dygie.models.global_analysis.clustering import *\n",
    "from dygie.models.global_analysis.relation_extraction import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dygie.training.evaluation import *\n",
    "dev_data = [json.loads(line) for line in open('../model_data/pwc_split_on_sectioned/dev.jsonl')]\n",
    "cluster_matching_thresholds = generate_thresholds(dev_data)\n",
    "print(cluster_matching_thresholds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dev_pred = [json.loads(line) for line in open('../outputs/test_results_folder/combined.jsonl')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_type_to_gold(x) :\n",
    "    x['n_ary_relations'] = [Relation(*y)._asdict() for y in x['n_ary_relations']]\n",
    "    x['true_entities'] = {}\n",
    "    for entity in used_entities :\n",
    "        x['true_entities'][entity] = list(set([rel[entity] for rel in x['n_ary_relations']]))\n",
    "        \n",
    "    x['gold_to_type'] = {}\n",
    "    for rel in x['n_ary_relations'] :\n",
    "        for k, v in rel.items() :\n",
    "            x['gold_to_type'][v] = k\n",
    "            \n",
    "    x['gold_clusters'] = {k:{'spans' : v, 'words' : [\" \".join(x['words'][y[0]:y[1]]) for y in v]} for k, v in x['coref'].items()}\n",
    "            \n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dev_pred = [json.loads(line) for line in open('../outputs/test_results_folder/combined.jsonl')]\n",
    "dev_data = [json.loads(line) for line in open('../model_data/pwc_split_on_sectioned/test.jsonl')]\n",
    "\n",
    "dev_data = {x['doc_id']:x for x in dev_data}\n",
    "for x in dev_pred :\n",
    "    x['true_coref'] = dev_data[x['doc_id']]['coref']\n",
    "    x['n_ary_relations'] = [Relation(*y)._asdict() for y in dev_data[x['doc_id']]['n_ary_relations']]\n",
    "    x['true_entities'] = {}\n",
    "    for entity in used_entities :\n",
    "        x['true_entities'][entity] = list(set([rel[entity] for rel in x['n_ary_relations']]))\n",
    "        \n",
    "    x['gold_to_type'] = {}\n",
    "    for rel in x['n_ary_relations'] :\n",
    "        for k, v in rel.items() :\n",
    "            x['gold_to_type'][v] = k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for d in dev_pred :\n",
    "    for k in d['true_coref'] :\n",
    "        if len(d['true_coref'][k]) == 0 :\n",
    "            print(d['doc_id'], k, d['true_coref'][k])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hand_annotation = {'007ff2ca5f297b04636699ce4d01ca6d6f21dc77' : { 'table' : ['Matched', 'Mismatched'] },\n",
    "                  '01c824989d24a8cae214c3156edd9d4492faa579' : { 'missed' : ['Conditional_Image_Generation']},\n",
    "                  '027d6c52be01b583b9a0d9eb8c9364c6b701c656' : { 'missed' : ['Percentage_correct']},\n",
    "                  '05d2700846c0323f79c1344aca5333994c7c03a5' : { 'missed' : ['IBM_2016']},\n",
    "                  '06c06885fd53b2cbd407704cf14f658842ed48e5' : { 'missed' : ['Set14_-_4x_upscaling', 'Set5_-_4x_upscaling']},\n",
    "                  '07a9478e87a8304fc3267fa16e83e9f3bbd98b27' : { 'missed' : ['Parameters']},\n",
    "                  '0910a4c470a410fac446f4026f7c8ef512ae7427' : { 'missed' : ['Percentage_correct']},\n",
    "                  '0a78873e41615798d09391d9f40d41666b8c9beb' : { 'missed' : ['Participant_Intervention_Comparison_Outcome_Extraction']}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for v in dev_pred :\n",
    "    v['missed'] = hand_annotation.get(v['doc_id'], {}).get('missed', [])\n",
    "    v['table'] = hand_annotation.get(v['doc_id'], {}).get('table', [])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for v in dev_pred :\n",
    "    v['prediction'] = {tuple(x['span']):x['label'] for x in v['prediction']}\n",
    "    v['gold'] = {tuple(x['span']):x['label'] for x in v['gold']}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for ex in dev_pred :\n",
    "    clusters, spl, cluster_labels = do_clustering(ex, 'prediction', 'coref_prediction', plot=True)\n",
    "    ex['clusters'] = clusters\n",
    "    ex['linked_clusters'] = get_linked_clusters(clusters)\n",
    "    relation_matrix = get_relations_between_clusters(ex, 'prediction', cluster_labels) #, ex['relation_threshold'])\n",
    "    ex['relation_matrix'] = relation_matrix\n",
    "    ex['predicted_relations'] = {n:generate_relations(clusters, relation_matrix, linked_clusters, n, ex['relation_threshold'])\n",
    "                                 for n in range(1, 6)}\n",
    "    \n",
    "    ex['gold_clusters'] = {k:{'spans' : v, 'words' : [\" \".join(ex['words'][y[0]:y[1]]) for y in v]} for k, v in ex['true_coref'].items()}\n",
    "    ex['true_relations'] = {n:generate_true_relations(ex['n_ary_relations'], n)\n",
    "                                 for n in range(1, 6)}\n",
    "    \n",
    "    map_all_clusters_to_true(ex, cluster_matching_thresholds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "p = 0\n",
    "for n in range(len(dev_pred)) :\n",
    "#     print(dev_pred[n]['linked_clusters'])\n",
    "    for rel in [Relation(**x) for x in dev_pred[n]['n_ary_relations']] :\n",
    "        print(rel)\n",
    "    print(\"-\"*30)\n",
    "    for v in dev_pred[n]['predicted_relations'][4].values() :\n",
    "        p += len(set(v))\n",
    "        for x in set(v) :\n",
    "            print([dev_pred[n]['clusters'][k]['matched'] if dev_pred[n]['clusters'][k]['matched'] is not None else dev_pred[n]['clusters'][k]['name']\n",
    "                   for k in x])\n",
    "\n",
    "    print(\"=\"*30)\n",
    "print(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# print(evaluate_for_all_document(dev_pred))\n",
    "\n",
    "oracle_res, oracle_exp_res, oracle_exp_res_table, gold_res, pred_res = run_eval_for_relation_extraction(dev_pred)\n",
    "x = []\n",
    "p = []\n",
    "g = []\n",
    "o = []\n",
    "oe = []\n",
    "oet = []\n",
    "oa = []\n",
    "\n",
    "for n, v in gold_res.items() :\n",
    "    if n < 5 :\n",
    "        print(n)\n",
    "        x.append(n)\n",
    "        print(pd.DataFrame(v))\n",
    "        g.append((pd.DataFrame(v).mean(1)*100).to_dict())\n",
    "        g[-1].update({\"n\" : str(n)})\n",
    "        print('-'*20)\n",
    "        print(pd.DataFrame(pred_res[n]))\n",
    "        p.append((pd.DataFrame(pred_res[n]).mean(1)*100).to_dict())\n",
    "        p[-1].update({\"n\" : str(n)})\n",
    "        print(\"=\"*20)\n",
    "        print(pd.DataFrame(oracle_res[n]))\n",
    "        o.append((pd.DataFrame(oracle_res[n]).mean(1)*100).to_dict())\n",
    "        o[-1].update({\"n\" : str(n)})\n",
    "        print(\"=\"*20)\n",
    "        oe.append((pd.DataFrame(oracle_exp_res[n]).mean(1)*100).to_dict())\n",
    "        oe[-1].update({\"n\" : str(n)})\n",
    "        oet.append((pd.DataFrame(oracle_exp_res_table[n]).mean(1)*100).to_dict())\n",
    "        oet[-1].update({\"n\" : str(n)})\n",
    "        oa.append((pd.DataFrame(oracle_abs[n]).mean(1)*100).to_dict())\n",
    "        oa[-1].update({\"n\" : str(n)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.DataFrame(p)\n",
    "data['type'] = 'predicted relation'\n",
    "\n",
    "data_1 = pd.DataFrame(g)\n",
    "data_1['type'] = 'gold relation'\n",
    "\n",
    "data_2 = pd.DataFrame(o)\n",
    "data_2['type'] = 'oracle relation'\n",
    "\n",
    "data_3 = pd.DataFrame(oe)\n",
    "data_3['type'] = 'oracle (expert) relation'\n",
    "\n",
    "data_4 = pd.DataFrame(oet)\n",
    "data_4['type'] = 'oracle (expert/table) relation'\n",
    "\n",
    "# data_5 = pd.DataFrame(oa)\n",
    "# data_5['type'] = 'abstract only'\n",
    "# data_5['f1'] = [33.1, 11.6, 3.4, 0.0]\n",
    "\n",
    "data = pd.concat([data, data_1, data_2, data_3, data_4]) #, data_5])\n",
    "\n",
    "sns.lineplot(x='n', y='f1', hue='type', data=data)\n",
    "plt.scatter(x=[3], y=[24.6], label=\"Manual Evaluation\")\n",
    "plt.scatter(x=[3], y=[8.69], label=\"Oracle (Abstract Only) Relation\", c='saddlebrown')\n",
    "plt.ylabel(\"F1-score\")\n",
    "plt.xlabel(\"N-ary Relation\")\n",
    "plt.gca().legend(loc='center left', bbox_to_anchor=(1, 0.5))\n",
    "sns.despine()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# from scripts.convert_predictions_to_brat import *\n",
    "# generate_brat_annotations(documents={x['doc_id']:x for x in dev_pred}, \n",
    "#                           brat_anno_folder='../brat/data/result_extraction_results/ner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scripts.result_from_pwc_table import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "map_to_arxiv = json.load(open('../data/arxiv_map.json'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for x in dev_pred :\n",
    "    x['arxiv_id'] = map_to_arxiv[x['doc_id']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for x in dev_pred :\n",
    "    if x['arxiv_id'] is not None :\n",
    "        print(x['doc_id'])\n",
    "        clusters = [x['words'] for x in x['gold_clusters'].values()]\n",
    "        print(get_scores(x['arxiv_id'], clusters))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_data = [add_type_to_gold(json.loads(line)) for line in open('../model_data/pwc_split_on_sectioned/train.jsonl')]\n",
    "# train_data += [add_type_to_gold(json.loads(line)) for line in open('../model_data/pwc_split_on_sectioned/dev.jsonl')]\n",
    "train_data = [add_type_to_gold(json.loads(line)) for line in open('../model_data/pwc_split_on_sectioned/test.jsonl')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for d in train_data :\n",
    "    map_gold_clusters_to_true(d, cluster_matching_thresholds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "mentions_in_doc = {k:defaultdict(int) for k in used_entities}\n",
    "matched_in_doc = {k:defaultdict(int) for k in used_entities}\n",
    "for ins in train_data :\n",
    "    for v, e in ins['gold_to_type'].items() :\n",
    "        if e in mentions_in_doc :\n",
    "            l = len(ins['gold_clusters'][v]['spans'])\n",
    "            mentions_in_doc[e][l if l < 1 else 1] += 1\n",
    "            \n",
    "            l1 = ins['gold_clusters'][v]['matched']\n",
    "            matched_in_doc[e][1 if l1 is not None else 0] += 1\n",
    "            if l > 0 and l1 is None :\n",
    "                print(v, ins['gold_clusters'][v])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for e, v in mentions_in_doc.items() :\n",
    "    print(e, v[1] / sum(v.values()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for e, v in matched_in_doc.items() :\n",
    "    print(e, v[1] / sum(v.values()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_gold, n_present = [], []\n",
    "for ins in train_data :\n",
    "    n_gold.append(len(ins['n_ary_relations']))\n",
    "    n_present.append(sum([all([len(ins['coref'][v]) > 0 for k, v in rel.items() if k != 'score']) for rel in ins['n_ary_relations']]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.scatter(n_gold, n_present)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum(n_present) / sum(n_gold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "x = [\"0\" if k == 0 else \" >0\" for e in mentions_in_doc for k, v in mentions_in_doc[e].items()]\n",
    "y = [v for e in mentions_in_doc for k, v in mentions_in_doc[e].items()]\n",
    "h = [e for e in mentions_in_doc for k, v in mentions_in_doc[e].items()]\n",
    "sns.barplot(x, y, h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "length = []\n",
    "for d in train_data :\n",
    "    for c, spans in d['coref'].items() :\n",
    "        for s in spans :\n",
    "            length.append(s[1] - s[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.countplot(length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum(np.array(length) > 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(np.linspace(1, 5, 5), 100 * (1 - 0.2)**np.linspace(1, 5, 5))\n",
    "plt.plot(np.linspace(1, 5, 5), 100 * (1 - 0.4)**np.linspace(1, 5, 5))\n",
    "plt.plot(np.linspace(1, 5, 5), 100 * (1 - 0.6)**np.linspace(1, 5, 5))\n",
    "plt.plot(np.linspace(1, 5, 5), 100 * (1 - 0.8)**np.linspace(1, 5, 5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dev_data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
