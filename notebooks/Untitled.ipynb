{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%config InlineBackend.figure_format = 'retina'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scripts.convert_brat_annotations_to_json import *\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from baseline.baseline import *\n",
    "from scripts.analyse_pwc_entity_results import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_concat = combine_brat_to_original_data('../data/pwc_s2_cleaned_text_v2.jsonl',\n",
    "                                         '../data/pwc_s2_cleaned_text_v2_sentences.jsonl',\n",
    "                                         '../outputs/pwc_s2_cleaned_text_v2_sentences_predictions.jsonl',\n",
    "                                         '~/brat/brat/data/result_extraction/outputs/brat_annotation_folder_doclevel_original/',\n",
    "                                         '~/brat/brat/data/result_extraction/outputs/brat_annotation_folder_doclevel/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from runtime_scripts.read_all_files import get_all_file_outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = get_all_file_outputs('../outputs/unannotated_results_folder/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.DataFrame(data.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(data['words'].apply(len) > 10000).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dygie.data.dataset_readers.read_pwc_dataset import *\n",
    "dump_all_to_file(read_dataframe(df_concat))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scripts.convert_pwc_to_brat import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "generate_brat_annotations('../data/pwc_s2_cleaned_text_v2.jsonl',\n",
    "                          '../data/pwc_s2_cleaned_text_v2_sentences.jsonl',\n",
    "                          '../outputs/pwc_s2_cleaned_text_v2_sentences_predictions.jsonl.clean',\n",
    "                          '../brat/data/result_extraction/outputs/second_phase_annotations_test/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [json.loads(line) for line in open('../model_data/pwc_split_on_sectioned/train.jsonl')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nr = 0\n",
    "nrt = 0\n",
    "for line in data :\n",
    "    n_ary_relations_all = [Relation(*x)._asdict() for x in line[\"n_ary_relations\"]]\n",
    "    corefs = {k: v for k, v in line['coref'].items() if len(v) > 0}\n",
    "    n_ary_relations = [\n",
    "        r for r in n_ary_relations_all if all([v in corefs for k, v in r.items() if k in used_entities])\n",
    "    ]\n",
    "    nr += len(n_ary_relations)\n",
    "    nrt += len(n_ary_relations_all)\n",
    "    \n",
    "nr, nrt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir('../')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sarthakj/miniconda3/lib/python3.7/site-packages/fuzzywuzzy/fuzz.py:11: UserWarning: Using slow pure-python SequenceMatcher. Install python-Levenshtein to remove this warning\n",
      "  warnings.warn('Using slow pure-python SequenceMatcher. Install python-Levenshtein to remove this warning')\n"
     ]
    }
   ],
   "source": [
    "from dygie.data.dataset_readers.pwc_json import PwCTagJsonReader\n",
    "from allennlp.data.token_indexers import PretrainedBertIndexer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "instances = list(PwCTagJsonReader({\"bert\" : PretrainedBertIndexer('bert-base-uncased')})._read('model_data/dataset_readers_paths/train.json:pwc'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "dev_instances = list(PwCTagJsonReader({\"bert\" : PretrainedBertIndexer('bert-base-uncased')})._read('model_data/dataset_readers_paths/dev.json:pwc'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dygie.data.iterators.batch_iterator import BatchIterator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "batched_instances = BatchIterator._shuffle_documents(instances, shuffle=False)\n",
    "dev_batched_instances = BatchIterator._shuffle_documents(dev_instances, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_span_features(instance) :\n",
    "    length = instance.fields['metadata'].metadata['document_metadata']['doc_length']\n",
    "    start_ix = instance.fields['metadata'].metadata['start_pos_in_doc']\n",
    "    spans_position = [(x.span_start, x.span_end) for x in instance.fields['spans'].field_list]\n",
    "    entity_labels = instance.fields['span_entity_labels'].labels\n",
    "    span_features = [x.labels for x in instance.fields['span_features'].field_list]\n",
    "    fdicts = []\n",
    "    \n",
    "    y = instance.fields['span_link_labels'].labels\n",
    "    new_y = []\n",
    "    for pos, elabel, f, l in zip(spans_position, entity_labels, span_features, y) :\n",
    "        if pos[0] > -1 :\n",
    "            fdict = {'Position' : (2*start_ix + pos[0] + pos[1] + 1)/(2*length), 'Entity_label' : elabel}\n",
    "            fdict.update({k:1 for k in f})\n",
    "            fdicts.append(fdict)\n",
    "            new_y.append(l)\n",
    "    \n",
    "    return fdicts, new_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "fdists, ys = [], []\n",
    "for ins in instances :\n",
    "    fdist, y = extract_span_features(ins)\n",
    "    fdists += fdist\n",
    "    ys += y\n",
    "    \n",
    "dev_fdists, dev_ys = [], []\n",
    "for ins in dev_instances :\n",
    "    dev_fdist, dev_y = extract_span_features(ins)\n",
    "    dev_fdists += dev_fdist\n",
    "    dev_ys += dev_y\n",
    "    \n",
    "from sklearn.feature_extraction.dict_vectorizer import DictVectorizer\n",
    "vec = DictVectorizer()\n",
    "vec.fit(fdists)\n",
    "fdists_vec = vec.transform(fdists)\n",
    "dev_fdists_vec = vec.transform(dev_fdists)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "lr = LogisticRegression(penalty='l2', class_weight='balanced')\n",
    "lr.fit(fdists_vec, ys)\n",
    "\n",
    "from dygie.training.thresholding_f1_metric import BinaryThresholdF1\n",
    "metrics = BinaryThresholdF1()\n",
    "metrics(lr.predict_proba(dev_fdists_vec)[:, 1], np.array(dev_ys))\n",
    "metrics.get_metric(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coref_labels = np.concatenate([x[0].fields['metadata'].metadata['document_metadata']['coref_labels'].reshape(-1) for x in batched_instances])\n",
    "coref_mask = np.concatenate([x[0].fields['metadata'].metadata['document_metadata']['coref_mask'].reshape(-1) for x in batched_instances])\n",
    "coref_features = np.concatenate([x[0].fields['metadata'].metadata['document_metadata']['coref_features'].reshape(-1, 13) for x in batched_instances])\n",
    "coref_labels = coref_labels[coref_mask.astype(bool)]\n",
    "coref_features = coref_features[coref_mask.astype(bool)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dev_coref_labels = np.concatenate([x[0].fields['metadata'].metadata['document_metadata']['coref_labels'].reshape(-1) for x in dev_batched_instances])\n",
    "dev_coref_mask = np.concatenate([x[0].fields['metadata'].metadata['document_metadata']['coref_mask'].reshape(-1) for x in dev_batched_instances])\n",
    "dev_coref_features = np.concatenate([x[0].fields['metadata'].metadata['document_metadata']['coref_features'].reshape(-1, 13) for x in dev_batched_instances])\n",
    "dev_coref_labels = dev_coref_labels[dev_coref_mask.astype(bool)]\n",
    "dev_coref_features = dev_coref_features[dev_coref_mask.astype(bool)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = LogisticRegression(class_weight='balanced')\n",
    "lr.fit(coref_features, coref_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dygie.training.thresholding_f1_metric import BinaryThresholdF1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = BinaryThresholdF1()\n",
    "metrics(lr.predict_proba(dev_coref_features)[:, 1], dev_coref_labels.astype(int))\n",
    "metrics.get_metric(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dygie.data.dataset_readers.entity_linking_reader import PwCLinkerReader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_span_corefs(instance) :\n",
    "    text = instance.fields['text'].tokens\n",
    "    spans = [(x.span_start, x.span_end+1) for x in instance.fields['spans'].field_list]\n",
    "    labels = [x.labels for x in instance.fields['span_coref_labels'].field_list]\n",
    "    entity_labels = instance.fields['span_entity_labels'].labels\n",
    "    map_coref_labels = {k:v for v, k in instance.fields['metadata'].metadata['map_coref_keys'].items()}\n",
    "    for span, label, el in zip(spans, labels, entity_labels) :\n",
    "        print(span, text[span[0]:span[1]], [map_coref_labels[x] for x in label], el)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import combinations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_pairs(instances) :\n",
    "    e_label = [label for instance in instances if 'spans' in instance.fields for label in instance.fields['span_entity_labels'].labels]\n",
    "    c_label = [label.labels for instance in instances if 'spans' in instance.fields for label in instance.fields['span_coref_labels'].field_list]\n",
    "    text = [\" \".join([x.text for x in ins.fields['text'].tokens[span.span_start:span.span_end + 1]]).lower() \n",
    "            for ins in instances if 'spans' in ins.fields for span in ins.fields['spans'].field_list]\n",
    "    p, q = 0, 0\n",
    "    for i, j in combinations(list(range(len(e_label))), 2) :\n",
    "        if i == j : continue\n",
    "        if e_label[i] != e_label[j] : continue\n",
    "        if len(set(c_label[i]) & set(c_label[j])) > 0 or text[i] == text[j]: \n",
    "            p += 1\n",
    "            q += 1\n",
    "            continue\n",
    "        if len(c_label[i]) == 0 and len(c_label[j]) == 0 : continue\n",
    "        if len(set(c_label[i]) & set(c_label[j])) == 0 : q += 1\n",
    "        \n",
    "    return p, q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum([len(ins.fields['spans'].field_list) for ins in instances])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x, y = zip(*[gen_pairs(x) for x in batched_instances])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum(x), sum(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pairs = PwCLinkerReader.generate_pairs('model_data/pwc_split_on_sectioned/train.jsonl')\n",
    "dev_pairs = PwCLinkerReader.generate_pairs('model_data/pwc_split_on_sectioned/dev.jsonl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_features, train_labels = zip(*[(x[-1], 1 if x[2] == 'Entailment' else 0) for x in pairs])\n",
    "dev_features, dev_labels = zip(*[(x[-1], 1 if x[2] == 'Entailment' else 0) for x in dev_pairs])\n",
    "train_features, dev_features = np.array(train_features), np.array(dev_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.where(np.array(dev_features) != dev_coref_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dev_features[0], dev_coref_features[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = LogisticRegression(class_weight='balanced')\n",
    "lr.fit(train_features, train_labels)\n",
    "\n",
    "metrics = BinaryThresholdF1()\n",
    "metrics(lr.predict_proba(train_features)[:, 1], np.array(train_labels))\n",
    "metrics.get_metric(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dev_coref_labels = np.array([z\n",
    "                            for x in dev_batched_instances \n",
    "                            for y in x[0].fields['metadata'].metadata['document_metadata']['coref_labels'] \n",
    "                            for z in y])\n",
    "\n",
    "dev_coref_mask = np.array([z == 1\n",
    "                            for x in dev_batched_instances \n",
    "                            for y in x[0].fields['metadata'].metadata['document_metadata']['coref_mask'] \n",
    "                            for z in y])\n",
    "\n",
    "dev_coref_labels[dev_coref_mask]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "Counter([x[2] for x in pairs])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum([x[0].fields['metadata'].metadata['document_metadata']['coref_labels'].shape[0] for x in batched_instances])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dygie.data.dataset_readers.span_utils import span_pair_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "span_pair_features((0, 2), (5, 6), \"SQuAD Dataset\", \"squad1.1\", {\"words\" : [0]*100})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run dygie/commands/predict_dygie.py outputs/pwc_outputs/experiment_dygie_crf_n_ary/testing_delete_2 outputs/pwc_outputs/experiment_linker/berty-bert-unfeatured/ model_data/dataset_readers_paths/dev.json:pwc outputs/n_ary_test 0"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
