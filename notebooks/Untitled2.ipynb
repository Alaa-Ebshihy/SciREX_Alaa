{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scripts.convert_brat_annotations_to_json import load_jsonl, used_entities\n",
    "def span_match(span_1, span_2) :\n",
    "    sa, ea = span_1\n",
    "    sb, eb = span_2\n",
    "    iou = (min(ea, eb) - max(sa, sb)) / (max(eb, ea) - min(sa, sb))\n",
    "    return iou"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ents = used_entities + ['None']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "annotated_data = load_jsonl('../model_data/all_data_propagated.jsonl')[:440]\n",
    "original_data = load_jsonl('../model_data/all_data_original_propagated.jsonl')[:440]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "for d in original_data :\n",
    "    span_to_cluster_ids = defaultdict(list)\n",
    "    for cluster_name in d['coref']:\n",
    "        for span in d['coref'][cluster_name]:\n",
    "            span_to_cluster_ids[tuple(span)].append(cluster_name)\n",
    "\n",
    "    span_to_cluster_ids = {span: set(sorted(v)) for span, v in span_to_cluster_ids.items()}\n",
    "    d['span_to_cluster'] = span_to_cluster_ids\n",
    "    \n",
    "for d in annotated_data :\n",
    "    span_to_cluster_ids = defaultdict(list)\n",
    "    for cluster_name in d['coref']:\n",
    "        for span in d['coref'][cluster_name]:\n",
    "            span_to_cluster_ids[tuple(span)].append(cluster_name)\n",
    "\n",
    "    span_to_cluster_ids = {span: set(sorted(v)) for span, v in span_to_cluster_ids.items()}\n",
    "    d['span_to_cluster'] = span_to_cluster_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def matching(spans_1, spans_2) :\n",
    "    match = np.zeros((len(spans_1), len(spans_2)))\n",
    "    match_types = {k + '_1':{l + '_2':0 for l in ents} for k in ents}\n",
    "    for i, s1 in enumerate(spans_1) :\n",
    "        for j, s2 in enumerate(spans_2) :\n",
    "#             if span_match((s1[0], s1[1]), (s2[0], s2[1])) > 0.5 :\n",
    "            if (s1[0], s1[1]) == (s2[0], s2[1]) :\n",
    "                match[i, j] = 1\n",
    "                \n",
    "    assert all(x <= 1 for x in match.sum(0)), breakpoint()\n",
    "    assert all(x <= 1 for x in match.sum(1)), breakpoint()\n",
    "    for i, s1 in enumerate(spans_1) :\n",
    "        matched = [j for j, x in enumerate(match[i]) if x == 1]\n",
    "        assert len(matched) <= 1\n",
    "        if len(matched) == 0 :\n",
    "            match_types[s1[2] + '_1']['None_2'] += 1\n",
    "        else :\n",
    "            match_types[s1[2] + '_1'][spans_2[matched[0]][2] + '_2'] += 1\n",
    "            \n",
    "            \n",
    "    for i, s2 in enumerate(spans_2) :\n",
    "        matched = [j for j, x in enumerate(match[:, i]) if x == 1]\n",
    "        assert len(matched) <= 1\n",
    "        if len(matched) == 0 :\n",
    "            match_types['None_1'][s2[2] + '_2'] += 1\n",
    "            \n",
    "    return match_types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "match_types = {k + '_1':{l + '_2':0 for l in ents} for k in ents}\n",
    "for i in tqdm(range(len(original_data))) :\n",
    "    match_types_doc = matching(original_data[i]['ner'], annotated_data[i]['ner'])\n",
    "    for k in match_types :\n",
    "        for l in match_types[k] :\n",
    "            match_types[k][l] += match_types_doc[k][l]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "match_types = pd.DataFrame(match_types).loc[[e + '_2' for e in ents], [e + '_1' for e in ents]].T / len(annotated_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(match_types.rename(index=lambda x : x.replace('_1', ''), columns=lambda x : x.replace('_2', '')).to_latex(float_format=\"{:0.1f}\".format))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "def matching_links(spans_1, spans_2, links_1, links_2) :\n",
    "    match = np.zeros((len(spans_1), len(spans_2)))\n",
    "    match_types = {k + '_2':{'Exist' : {'+' : 0, '-' : 0}, 'New' : {'+' : 0, '-' : 0}} for k in ents}\n",
    "    for i, s1 in enumerate(spans_1) :\n",
    "        for j, s2 in enumerate(spans_2) :\n",
    "            if span_match((s1[0], s1[1]), (s2[0], s2[1])) > 0.5 :\n",
    "                match[i, j] = 1\n",
    "                \n",
    "    assert all(x <= 1 for x in match.sum(0)), breakpoint()\n",
    "    assert all(x <= 1 for x in match.sum(1)), breakpoint()\n",
    "            \n",
    "            \n",
    "    for i, s2 in enumerate(spans_2) :\n",
    "        matched = [j for j, x in enumerate(match[:, i]) if x == 1]\n",
    "        assert len(matched) <= 1\n",
    "        is_matched = True if len(matched) > 0 and spans_1[matched[0]][2] == s2[2] else False\n",
    "        if not is_matched :\n",
    "            t = 'New'\n",
    "            new_links = links_2.get(tuple(s2[:2]), set())\n",
    "            match_types[s2[2] + '_2'][t]['+'] = len(new_links)\n",
    "        else :\n",
    "            t = 'Exist'\n",
    "            matched_span = spans_1[matched[0]]\n",
    "            new_links, old_links = links_2.get(tuple(s2[:2]), set()), links_1.get(tuple(matched_span[:2]), set())\n",
    "            match_types[s2[2] + '_2'][t]['+'] += len(new_links - old_links)\n",
    "            match_types[s2[2] + '_2'][t]['-'] += len(old_links - new_links)\n",
    "            \n",
    "            \n",
    "    return match_types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "match_types = {k + '_2':{(t, q) : 0 for t in ['Exist', 'New'] for q in ['+', '-']} for k in ents}\n",
    "\n",
    "for i in tqdm(range(len(annotated_data))) :\n",
    "    m = matching_links(original_data[i]['ner'], annotated_data[i]['ner'], original_data[i]['span_to_cluster'], annotated_data[i]['span_to_cluster'])\n",
    "    for k in m :\n",
    "        m[k] = {(s, x):n for s, v in m[k].items() for x, n in v.items()}\n",
    "        for (s, x), n in m[k].items() :\n",
    "            match_types[k][(s, x)] += n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print((pd.DataFrame(match_types).T / len(annotated_data)).rename(index=lambda x : x.replace('_2', '')).to_latex(float_format=\"{:0.1f}\".format))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dygie.data.dataset_readers.read_pwc_dataset import is_x_in_y\n",
    "does_overlap = lambda x, y: max(x[0], y[0]) < min(x[1], y[1])\n",
    "\n",
    "def map_to_section_and_sentence(data) :\n",
    "    mentions = data['ner']\n",
    "    sections = data['sections']\n",
    "    sentences = data['sentences']\n",
    "    \n",
    "    for e in mentions:\n",
    "        in_sentences = [i for i, s in enumerate(data[\"sentences\"]) if is_x_in_y(e, s)]\n",
    "        if len(in_sentences) > 1:\n",
    "            breakpoint()\n",
    "        if len(in_sentences) == 0:\n",
    "            in_sentences = [i for i, s in enumerate(data[\"sentences\"]) if does_overlap(e, s)]\n",
    "            assert sorted(in_sentences) == list(range(min(in_sentences), max(in_sentences) + 1)), breakpoint()\n",
    "            # breakpoint()\n",
    "            in_sentences = sorted(in_sentences)\n",
    "            data[\"sentences\"][in_sentences[0]][1] = data[\"sentences\"][in_sentences[-1]][1]\n",
    "            data[\"sentences\"] = [s for i, s in enumerate(data[\"sentences\"]) if i not in in_sentences[1:]]\n",
    "    mention_to_section = {}\n",
    "    mention_to_sentence = {}\n",
    "    for s, e, _ in mentions :\n",
    "        done = False\n",
    "        for s_1, e_1 in sections :\n",
    "            if is_x_in_y((s, e), (s_1, e_1)):\n",
    "                mention_to_section[(s, e)] = (s_1, e_1)\n",
    "                done = True\n",
    "        assert done\n",
    "        \n",
    "    for s, e, _ in mentions :\n",
    "        done = False\n",
    "        for s_1, e_1 in sentences :\n",
    "            if is_x_in_y((s, e), (s_1, e_1)):\n",
    "                mention_to_sentence[(s, e)] = (s_1, e_1)\n",
    "                done = True\n",
    "        assert done\n",
    "        \n",
    "    data['mention_to_section'] = mention_to_section\n",
    "    data['mention_to_sentence'] = mention_to_sentence\n",
    "    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "for d in tqdm(annotated_data) :\n",
    "    map_to_section_and_sentence(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scripts.entity_utils import used_entities\n",
    "from itertools import combinations\n",
    "def stats_for_relation(data) :\n",
    "    n_ary_relations = data['n_ary_relations']\n",
    "    corefs = data['coref']\n",
    "    stats = {}\n",
    "    for card in range(2, 5) :\n",
    "        for elist in combinations(used_entities, card) :\n",
    "            stats[tuple(elist)] = {'In section':0, 'Out section':0, 'In sentence':0, 'Out sentence':0}\n",
    "            for rel in n_ary_relations :\n",
    "                rel = [rel[k] for k in elist]\n",
    "                clusters = [corefs[x] for x in rel]\n",
    "                if any(len(c) == 0 for c in clusters) :\n",
    "                    continue\n",
    "                sections = [set([data['mention_to_section'][tuple(span)] for span in cluster]) \n",
    "                            for cluster in clusters]\n",
    "                sections = set.intersection(*sections)\n",
    "                if len(sections) > 0 :\n",
    "                    stats[tuple(elist)]['In section'] += 1\n",
    "                else :\n",
    "                    stats[tuple(elist)]['Out section'] += 1\n",
    "                    if card == 4 :\n",
    "                        print(data['doc_id'], rel)\n",
    "                    \n",
    "                sections = [set([data['mention_to_sentence'][tuple(span)] for span in cluster]) \n",
    "                            for cluster in clusters]\n",
    "                sections = set.intersection(*sections)\n",
    "                if len(sections) > 0 :\n",
    "                    stats[tuple(elist)]['In sentence'] += 1\n",
    "                else :\n",
    "                    stats[tuple(elist)]['Out sentence'] += 1\n",
    "                    \n",
    "    return stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "stats = stats_for_relation(annotated_data[0])\n",
    "for d in tqdm(annotated_data[1:]) :\n",
    "    stats_d = stats_for_relation(d)\n",
    "    for key in stats_d :\n",
    "        for v, x in stats_d[key].items() :\n",
    "            stats[key][v] += x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "stats = {\"/\".join(k):v for k, v in stats.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = pd.DataFrame(stats).T / 440"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "metrics['n_ary'] = pd.Series(metrics.index).apply(lambda x : len(x.split('/'))).values\n",
    "print(metrics.groupby('n_ary').agg(np.mean).to_latex(float_format=lambda x: \"{:0.3f}\".format(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
